{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- During data munging, I remove outlier (only TOTAL removed) and remove missing data (Removed data that did not have financial data). I removed all data that did not have 'salary' information. Rationale for doing so was that a classifer algorithm can identify missing data as a feature itself. As the number of POIs with missing data was minimal (i think 1), I thought it was ok to remove data without financial information. Also if you dont remove financial data, and impute missing values by 0, You will get an algorithm that has high precision, because of all the people in missing data, only 1 person is POI, rest are non-POIs. Do not impute median for missing data. This is not a good strategy for this data set. Reasons below, \n",
    "\n",
    "1.1- Many employees dont get additional financial incentives like bonus, restricted stock etc, so its not good to put median values for those people, its better to imput 0. Mainly because a lower level employee may actually be getting 0 additional incentives, and that data may not have been included in the data set.\n",
    "\n",
    "1.2- About 60 data points have email features alone, and do not have financial data. I removed them completely. But putting median values for them may not be correct representation of data. For people with missing financial features, only 1 or 2 are POIs, so including or removing them, only increases or decreases precision and doesn't affect the overall approach in anyway. \n",
    "\n",
    "I only included data that had entries for salary. The ones that did not have salary information had only email features are removed. There was a lesson about choosing data from different sources, so I excluded ones with email completely, to avoid issues due to data from different sources.\n",
    "\n",
    "2- Applied log(1+data) to financial information, to get more 'normal' distribution. I computed ratios for email features and added two new email radio features. \n",
    "\n",
    "3- Did PCA on financial features and selected 3 best. (PCA on training data only). (3- obtained using grid search CV)\n",
    "\n",
    "3.1- Do not take PCAs on combined financial and email data. This is troublesome due to a lot of missing data, futher a hybrid email and financial data is tough to visualize for me, so I kept them separate. \n",
    "\n",
    "3.2- I chose 3 PCA components, because they explained 70% of variance in data for the features. 3-4 principal components may be sufficient to capture most of the variance in the data.\n",
    "\n",
    "3.3- Do not select \"total_payments\" and \"total_stock_incentives\", because they are perfectly correlated with other features. \n",
    "\n",
    "4- The default 0.3 split from sklearn's cross validation function is used. In general smaller test_size implies overfitting. However, we are repeating this for all data points, so we are in a way using all the data. Further, in cases like this where the number of samples is low, using lower split actually avoids overfitting. Because in modeling, we are predicting only 1 data point, so the model has to be general enough to predict that 1 data sample that wasn't included in the original modeling. This is one of the reasons why when having test size of 0.3, we have high scores.\n",
    "\n",
    "5- Applied gridsearchCV with custom scoring function and DecisionTree classifier. (I varied criterion and splitter). I do not use F1 score. Instead, I create my own scoring function. My scoring function was to maximize the minimum of precision and recall. This helped improve the performance significantly. F1 score is harmonic mean of precision and recall, so it tries to balance the two. It doesn't maximize either, but tries to keep both high and equal to one another. F1 = 2/(1/recall + 1/precision). I use make_scorer in sklearn to make custom scoring function.\n",
    "\n",
    "6- For cross-validation I used the same tester function as that in tester.py. Sklearn's cross validation function uses the default 0.3 split. About using smaller test size, in general smaller test_size implies overfitting. However, we can repeat this for all data points, so we are in a way using all the data. Further, in cases like this where the number of samples is low, using lower split actually avoids overfitting. Because in modeling, we are predicting only 1 data point, so the model has to be general enough to predict that 1 data sample that wasn't included in the original modeling. This is one of the reasons why when we have test size of 0.3, you have high scores. \"precision_recall\" is a function I wrote to calculate precision and recall. It returns precision and recall from precitions and actual value. I thinkm, due to large difference in number of POIs and non-POIs,  18 pois in 150 people, when test_size is 0.1, we are predicting 2 or 1 POIs among 15, with test_sizxe is 0.3, we have 3 to 6 POIs in testing set. So even we only get 2 of them right, we have precision and recall above 0.3. So the scores appear artifitally high for a poorer model, like grade inflation. With 0.1, the model has to be generalize well to predict 1 data point, and any error is amplified. Regardless, it makes more sense to use 0.1 for test_size in gridsearchCV because thats what is used in tester.py. Also, the parameters of model are calculated based on CV object provided to gridsearchCV, so the parameters that give best score for 0.1 test size may be different from those for 0.3\n",
    "\n",
    "Here is what I do:\n",
    "6.1- We first divide the data set into a training and testing set of, say, 70%-30%.\n",
    "\n",
    "6.2- On the 70% of the data that is put in the training set we then run the grid_search to train the model using the StratifiedShuffleSplit. Within the 'sss' the data is again randomly divided into 90-10 percent training-test sets for 1000 iterations, each iteration trains a model on 90% of the training set and tests on 10% of the original training set.\n",
    "\n",
    "6.3- Then, the model with the parameters that have performed best on average, is used to generate predictions on the the 30% of the original data set that was set aside for testing. The predictions of that model are then compared to the actual values of the dependent variable to score the final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import pprint\n",
    "import math\n",
    "import matplotlib.pyplot\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "import datetime\n",
    "sys.path.append(\"../ud120-projects/tools/\")\n",
    "sys.path.append(\"../ud120-projects/final_project/\")\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.cross_validation import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Select what features you'll use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features_list is a list of strings, each of which is a feature name.\n",
    "The first feature must be \"poi\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poi', 'salary', 'bonus']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list = ['poi','salary', 'bonus']\n",
    "features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dictionary containing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../ud120-projects/final_project/final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "#pp.pprint(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points:  146\n",
      "Total number of POI:  18  Percentage of POI:  12.33 %\n",
      "Total number of non-POI:  128  Percentage of non-POI:  87.67 %\n"
     ]
    }
   ],
   "source": [
    "total_number = len(data_dict)\n",
    "total_poi_number = sum( x['poi'] == True for x in data_dict.values() )\n",
    "print \"Total number of data points: \", total_number\n",
    "print \"Total number of POI: \", total_poi_number, \" Percentage of POI: \", \\\n",
    "       round(total_poi_number / (1.0 * total_number), 4) * 100, \"%\"\n",
    "print \"Total number of non-POI: \", total_number - total_poi_number, \\\n",
    "       \" Percentage of non-POI: \", (1 - round(total_poi_number / (1.0 * total_number), 4))* 100, \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of missing values for feature ' total_payments ' is:  21\n",
      "Total number of missing values for feature ' total_stock_value ' is:  20\n"
     ]
    }
   ],
   "source": [
    "my_chosen_feature_list = ['total_payments', 'total_stock_value']\n",
    "for i in my_chosen_feature_list:\n",
    "    print \"Total number of missing values for feature '\", i, \"' is: \", \\\n",
    "    sum( x[i] == \"NaN\" for x in data_dict.values() )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will at first remove all data that did not have 'salary' information. In other words, I will remove the person whose 'salary' is 'NaN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict_no_NaN = {name:data_dict[name] for name in data_dict if data_dict[name][\"salary\"] != \"NaN\"}\n",
    "len(data_dict_no_NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_dict_no_NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new function draw_scatterplot() is created here to draw the scatter plot for the data. This visualizes where the outliers are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Red dots in the scatter plot are POI's \n",
    "# Blue dots are not POI's\n",
    "def draw_scatterplot(featurelist, datadict):\n",
    "    data = featureFormat(datadict, featurelist)\n",
    "    for point in data:\n",
    "        if point[0] != 0:\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'blue'\n",
    "        matplotlib.pyplot.scatter( point[1], point[2], color = color)\n",
    "    matplotlib.pyplot.xlabel( featurelist[1] )\n",
    "    matplotlib.pyplot.ylabel( featurelist[2] )\n",
    "    matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the scatter plot for all data. Let us see the outliers. I am using 'salary' and 'bonus' as the features to study. As I mentioned in the code above, the red dots in the scatter plot are POI's, and the blue dots in the scatter plot are non-POI's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEVCAYAAADOwrOnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKAI1PgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9f4bdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_list = ['poi','salary', 'bonus']\n",
    "draw_scatterplot(features_list, data_dict_no_NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scatter plot above, a blue dot is found with much bigger bonus and salary. Now I need to find out who this is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function Remove_Max_Outlier() to remove the outliers from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Remove_Max_Outlier(datadict, feature):\n",
    "    v=list(k[feature] for k in datadict.values())\n",
    "    k=list(datadict.keys())\n",
    "    name = k[v.index(max(v))]\n",
    "    print name\n",
    "    print datadict[name][feature]\n",
    "\n",
    "    del datadict[name]\n",
    "    return datadict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run this function to remove the outlier. It also print out some information about this outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL\n",
      "309886585\n"
     ]
    }
   ],
   "source": [
    "data_dict_no_Outlier = Remove_Max_Outlier(data_dict_no_NaN, \"total_payments\")\n",
    "#data_dict_no_Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a scatter plot to see which outliers are removed. As I mentioned before, the red dots are POI's. the blue dots are non-POI's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEVCAYAAADD3MPgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9f4b860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_scatterplot(features_list, data_dict_no_Outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Change the value of data to get more normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #1, Apply log (1+data) to financial features in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applied log(1+data) to financial information, and to email information to get more 'normal' distribution. I computed ratios and log-transformed them for email features, and took log of financial data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to set all data to be zero, if it is NaN. To do this, I define a function NaN_to_Zero()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NaN_to_Zero(featurelist, datadict):\n",
    "    for key, value in datadict.items():\n",
    "        for i in featurelist:\n",
    "            if type(value[i]) is float and math.isnan(value[i]) or type(value[i]) is str and value[i] == \"NaN\" :\n",
    "                datadict[key][i] = 0\n",
    "    return datadict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I call this function to change any \"NaN\" value in the data_dict_no_Outlier to zero for the listed features. Feature 'deferred_income' and 'restricted_stock_deferred' are not taken into consideration because it has negative values, and we not able to apply log calculation on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_list = ['bonus', 'deferral_payments', 'director_fees', 'exercised_stock_options', \\\n",
    "                 'expenses', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', \\\n",
    "                 'salary', 'total_payments', 'total_stock_value']\n",
    "data_dict_no_NaN2 = NaN_to_Zero(features_list, data_dict_no_Outlier)\n",
    "#data_dict_no_NaN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name in data_dict_no_NaN2:\n",
    "    data_point = data_dict_no_NaN2[name]\n",
    "    for i in features_list:\n",
    "        data_point[i] = math.log(float(data_point[i]) + 1.0)\n",
    "#data_dict_no_NaN2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #2, Calculate ratios to create the new email features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will create two new email features \"fraction_from_poi\" and \"fraction_to_poi\". As you may know, feature \"fraction_from_poi\" is the result of feature \"from_this_person_to_poi\" divided by feature \"from_messages\". And feature \"fraction_to_poi\" is the result of feature \"from_poi_to_this_person\" divided by feature \"to_messages\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I define the function computeFraction() to do the calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeFraction( poi_messages, all_messages ):\n",
    "    \"\"\" given a number messages to/from POI (numerator) \n",
    "        and number of all messages to/from a person (denominator),\n",
    "        return the fraction of messages to/from that person\n",
    "        that are from/to a POI\n",
    "   \"\"\"\n",
    "    ### this function returns either\n",
    "    ###     the fraction of all messages to this person that come from POIs\n",
    "    ###     or\n",
    "    ###     the fraction of all messages from this person that are sent to POIs\n",
    "    ### the same code can be used to compute either quantity\n",
    "\n",
    "    ### beware of \"NaN\" when there is no known email address (and so\n",
    "    ### no filled email features), and integer division!\n",
    "    ### in case of poi_messages or all_messages having \"NaN\" value, return 0.\n",
    "    fraction = 0.\n",
    "    if poi_messages == \"NaN\" or all_messages == \"NaN\" or all_messages == 0:\n",
    "        return fraction\n",
    "    fraction = 1.0 * poi_messages / all_messages\n",
    "    return fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the two new features \"fraction_from_poi\" and \"fraction_to_poi\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in data_dict_no_NaN2:\n",
    "\n",
    "    data_point = data_dict_no_NaN2[name]\n",
    "\n",
    "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
    "    to_messages = data_point[\"to_messages\"]\n",
    "    fraction_from_poi = computeFraction( from_poi_to_this_person, to_messages )\n",
    "    data_point[\"fraction_from_poi\"] = fraction_from_poi\n",
    "\n",
    "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
    "    from_messages = data_point[\"from_messages\"]\n",
    "    fraction_to_poi = computeFraction( from_this_person_to_poi, from_messages )\n",
    "    data_point[\"fraction_to_poi\"] = fraction_to_poi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_dict_no_NaN2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here let us look at the scatter plot from the two new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEQCAYAAACX5IJuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4v8DQWRO064nvUcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9f4b208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_list = ['poi', 'fraction_from_poi', 'fraction_to_poi']\n",
    "\n",
    "draw_scatterplot(features_list, data_dict_no_NaN2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this scatterplot, we can see that POI stays at an area where fracton_from_poi is in 0 to 0.15, and fraction_to_poi is in 0.1 to 0.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store to my_dataset for easy export below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dataset = data_dict_no_NaN2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Select the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the study, I will only select the financial features. And among the financial features some features can not be used. These unused features are:<br/>\n",
    "1, \"Total Payments\" and \"Total Stock Values\". The values of both features are calculated from other financial features.<br/>\n",
    "2, \"Deferred Income\" and \"Restricted Stock Deferred\". The values of both features are negative. It requires some special handling on these negative values. For now I choose to ignore them.<br/>\n",
    "\n",
    "So the features list is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_list = ['poi', 'bonus', 'deferral_payments', 'director_fees', 'exercised_stock_options', \\\n",
    "                 'expenses', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', \\\n",
    "                 'salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the dataset for the selected features above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,  15.24462516,  14.86972433, ...,   5.03043792,\n",
       "         11.74425938,  12.21580513],\n",
       "       [  0.        ,   0.        ,   0.        , ...,  13.66993435,\n",
       "         14.37943306,   6.16961073],\n",
       "       [  0.        ,  13.99783295,  14.07459175, ...,  14.79395096,\n",
       "         15.18738013,  12.49538963],\n",
       "       ..., \n",
       "       [  0.        ,  14.91412318,   0.        , ...,  12.61495524,\n",
       "         14.84376404,  13.14288143],\n",
       "       [  0.        ,  13.01700508,   0.        , ...,  13.8975181 ,\n",
       "         16.44358458,  12.66837167],\n",
       "       [  1.        ,   0.        ,   0.        , ...,  11.90463642,\n",
       "         15.0898133 ,  11.97290401]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = featureFormat(my_dataset, features_list, remove_all_zeroes=False, sort_keys = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the features, there are two ways:<br/>\n",
    "1, Select the features manually.  I write the code to loop through the features and find which onces are important.<br\\>\n",
    "2, Select the features by calling RandomizedPCA().\n",
    "\n",
    "I tried both. And I choose finaly to select the features manually. I will show the reason later.\n",
    "\n",
    "When doing PCA, I find that I need to select 3 components to preserve most of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca.explained_variance_ratio_: \n",
      "[ 0.260902    0.22110866  0.17130492]\n",
      "first_pc =  [  6.70524909e-06  -1.34298249e-02  -5.56515854e-02   1.74564542e-18\n",
      "  -9.85834553e-02   1.26412139e-02  -1.13413421e-02  -8.20958521e-02\n",
      "  -2.02554869e-02  -1.48927064e-02  -3.09277097e-03] Length =  11\n",
      "second_pc =  [ -1.80797177e-03  -5.37291522e-02   9.32031081e-02   4.33424848e-18\n",
      "   3.95376707e-02  -1.40579780e-02   5.47763421e-03  -8.75289072e-02\n",
      "  -3.59410639e-02  -4.50521813e-02  -5.89138272e-03] Length =  11\n",
      "third_pc =  [ -5.79782558e-04  -1.33458753e-02   7.75166421e-02   1.96966221e-17\n",
      "  -1.08554428e-01  -6.16175591e-02   9.43177124e-04   8.02385767e-02\n",
      "   2.35635511e-03  -5.70576074e-02  -2.54897298e-03] Length =  11\n"
     ]
    }
   ],
   "source": [
    "pca = RandomizedPCA(n_components=3, whiten=True)\n",
    "pca.fit(data)\n",
    "\n",
    "print \"pca.explained_variance_ratio_: \"\n",
    "print pca.explained_variance_ratio_\n",
    "print \"first_pc = \", pca.components_[0], \"Length = \", len(pca.components_[0])\n",
    "print \"second_pc = \", pca.components_[1], \"Length = \", len(pca.components_[1])\n",
    "print \"third_pc = \", pca.components_[2], \"Length = \", len(pca.components_[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Select the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried four algorithms: GaussionNB, DecisionTree, RandomForest, and SVC.\n",
    "\n",
    "I will show it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Preparation: Define the custom Scoring function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to apply gridsearchCV() with a custom scoring function I define and alogrithm classifier. My scoring function was to maximize the minimum of precision and recall, instead of F1-score. This helps improve the performance significantly. F1 score is harmonic mean of precision and recall, so it tries to balance the two. It doesn't maximize either, but tries to keep both high and equal to one another. F1 = 2/(1/recall + 1/precision). So I do not choose F1-score. Instead, I choose to use the minimum of the precision and recall. I use make_scorer in sklearn to make custom scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision_recall(labels, predictions):\n",
    "    ind_true_pos = [i for i in range(0,len(labels)) if (predictions[i]==1) & (labels[i]==1)]\n",
    "    ind_false_pos = [i for i in range(0,len(labels)) if ((predictions[i]==1) & (labels[i]==0))]\n",
    "    ind_false_neg = [i for i in range(0,len(labels)) if ((predictions[i]==0) & (labels[i]==1))]\n",
    "    ind_true_neg = [i for i in range(0,len(labels)) if ((predictions[i]==0) & (labels[i]==0))]\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    \n",
    "    ind_labels = [i for i in range(0,len(labels)) if labels[i]==1]\n",
    "    \n",
    "    if len(ind_labels) !=0:\n",
    "        if float( len(ind_true_pos) + len(ind_false_pos))!=0:\n",
    "            precision = float(len(ind_true_pos))/float( len(ind_true_pos) + len(ind_false_pos))\n",
    "        if float( len(ind_true_pos) + len(ind_false_neg))!=0:\n",
    "            recall = float(len(ind_true_pos))/float( len(ind_true_pos) + len(ind_false_neg))\n",
    "        return precision, recall\n",
    "    else:\n",
    "        return -1,-1\n",
    "\n",
    "def custom_scorer(labels, predictions):\n",
    "    precision, recall = precision_recall(labels, predictions)\n",
    "    min_score = min(precision, recall)\n",
    "    return min_score\n",
    "score  = make_scorer(custom_scorer, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Preparation: Cross-validation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cross-validation I used the same tester functions train_test_split() and StratifiedShuffleSplit() as that in tester.py.<br\\>\n",
    "\n",
    "Here is what I am going to do:<br\\>1, we first divide the data set into a training and testing set of, say, 70%-30%.\n",
    "On the 70% of the data that is put in the training set we then run the grid_search to train the model using the StratifiedShuffleSplit.<br\\>2, Within the 'sss', the data is again randomly divided into 90%-10% training-test sets for 1000 iterations. Each iteration trains a model on 90% of the training set and tests on 10% of the original training set.<br\\>\n",
    "3, Then, the model with the parameters that have performed best on average, is then used to generate predictions on the the 30% of the original data set that was set aside for testing.<br\\>4,The predictions of that model are then compared to the actual values of the dependent variable to score the final model.<br\\>\n",
    "\n",
    "sklearn's cross validation function uses the default 0.3 split. But using smaller test_size, in general, implies overfitting. However, you can repeat this for all data points, so you are in a way using all the data. Further, in cases like this where the number of samples is low, using lower split actually avoids overfitting. Because in modeling, you are predicting only 1 data point, so the model has to be general enough to predict that 1 data sample that wasn't included in the original modeling. This is one of the reasons why when you have test size of 0.3, you have high scores.<br\\> This is my reasoning for 0.1 vs 0.3.<br\\>\n",
    "In other words, this is due to large difference in number of POIs and non-POIs. 18 POIs in 150 people. When test_size is 0.1, you are predicting 2 or 1 POIs among 15, where as when it is done with 0.3, you have 3 to 6 POIs in testing set. So even you get 2 of them right, you have precision and recall above 0.3. So the scores appear artifitally high for a poorer model, like grade inflation. With 0.1, the model has to be generalize well to predict 1 data point, and any error is amplified. Regardless, it makes more sense to use 0.1 for test_size in gridsearchCV because that is what is used in tester.py. Also, the parameters of model are calculated based on CV object provided to gridsearchCV, so the parameters that give best score for 0.1 test size may be different from those for 0.3.<br\\>\n",
    "\n",
    "You will see how I call train_test_split() and StratifiedShuffleSplit(), as well as GridSearchCV() later in my code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have a output function get_outcomes(). It prints out all important informations from grid_search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_outcomes(gridCV):\n",
    "    '''Gets the print out of all the outcomes from the grid_search. It prints out the \n",
    "    best parameters found by the model and the outcomes of the test of the model on \n",
    "    the test set.'''\n",
    "    print \"Best parameters from the grid search: \", gridCV.best_params_\n",
    "    clf_gridCV = gridCV.best_estimator_\n",
    "    print \"\\nBest Estimator Accuracy:\", clf_gridCV.score(features_test, labels_test)\n",
    "    clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "    print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)\n",
    "    print \"\\n\\nPrecision Score:\", precision_score(labels_test, clf_gridCV_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried several different algorithms with one of two ways to select features. I list them here:<br\\>\n",
    "1, PCA + SVC + Pipeline + GridSearchCV<br\\>\n",
    "2, PCA + DecisionTree + Pipeline + GridSearchCV<br\\>\n",
    "3, PCA + RandomForest + Pipeline + GridSearchCV<br\\>\n",
    "4, manually selecting features + DecisionTree + GridSearchCV<br\\>\n",
    "5, manually selecting features + RandomForest + GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8.1: PCA + SVC + Pipeline + GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation of this solution takes a very long time. So I have to run one EC2 instance \"r3.8xlarge\" in Amazon Web Service (AWS). This interface have 32 virtual CPUs and 244G memory. It takes about 36 hours to finish the calculation. To fully use the 32 virtual CPUS, I need to change the code to use threads to do the calculation: Run one thread for one set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "def get_outcomes2(gridCV, file_handle):\n",
    "\n",
    "    '''Gets the print out of all the outcomes from the grid_search. It prints out the \n",
    "    best parameters found by the model and the outcomes of the test of the model on \n",
    "    the test set.'''\n",
    "    \n",
    "    file_handle.write(\"\\n\\nBest parameters from the grid search: \" + str(gridCV.best_params_))\n",
    "    clf_gridCV = gridCV.best_estimator_\n",
    "    file_handle.write(\"\\n\\nBest Estimator Accuracy:\" + str(clf_gridCV.score(features_test, labels_test)))\n",
    "    clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "    file_handle.write(\"\\n\\nRecall Score:\" + str(recall_score(labels_test, clf_gridCV_pred)))\n",
    "    file_handle.write(\"\\n\\nPrecision Score:\" + str(precision_score(labels_test, clf_gridCV_pred)))\n",
    "            \n",
    "sss = StratifiedShuffleSplit(labels_train, n_iter=1000, test_size=0.1, random_state=42)                       \n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('pca', RandomizedPCA()),\n",
    "        ('clf', SVC(random_state = 42))\n",
    "    ])\n",
    "\n",
    "def worker(xi, di, si, mi):\n",
    "    folder = \"./pipeline_svc_thread_result/test_result_xi_\" + str(xi[0]) + \"_di_\" + str(di[0]) + \"_si_\" + str(si[0]) + \"_mi_\" + str(mi[0]) + \"/\"\n",
    "\n",
    "    dir = os.path.dirname(folder)\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    f = open(folder + \"result.txt\", \"a+\")\n",
    "\n",
    "    param_grid = {'pca__n_components': xi,\n",
    "              'clf__C': ci,\n",
    "              'clf__gamma': gi,\n",
    "              'clf__kernel': di}\n",
    "\n",
    "    clf = GridSearchCV(estimator = Pipeline, \n",
    "                             param_grid = param_grid,\n",
    "                             scoring = score,\n",
    "                             cv = sss)\n",
    "\n",
    "    f.write(str(clf))\n",
    "\n",
    "    f.write(\"\\n\\nTraining starts at: \" + datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    clf.fit(features_train, labels_train)\n",
    "    f.write(\"\\n\\nTraining ends at: \" + datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# print all the outcomes of interest\n",
    "    get_outcomes2(clf, f)\n",
    "    my_features_list = features_list\n",
    "\n",
    "    dump_classifier_and_data(folder, clf, my_dataset, features_list)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "x = [[3]]\n",
    "d = [['rbf'],['linear'], ['poly']]\n",
    "c = [[10], [100], [500]]\n",
    "g = [[0.01], [0.1], [1]]\n",
    "\n",
    "for xi in x:\n",
    "    for di in d:\n",
    "        for ci in c:\n",
    "            for gi in g:\n",
    "                t = threading.Thread(target=worker, args=(xi, di, ci, gi))\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However its result is not good. The value of recall never goes beyond 0.3 for different paramter settings. Its maximum value is 0.2857. The best result is:\n",
    "\n",
    "Training starts at: 2016-08-31 11:10:21\n",
    "\n",
    "Training ends at: 2016-09-01 17:22:27\n",
    "\n",
    "Best parameters from the grid search: {'clf__gamma': 1, 'pca__n_components': 3, 'clf__C': 10, 'clf__kernel': 'poly'}\n",
    "\n",
    "Best Estimator Accuracy:0.793103448276\n",
    "\n",
    "Recall Score:0.285714285714\n",
    "\n",
    "Precision Score:0.66666666666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8.2: PCA + RandomForest + Pipeline + GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Pipeline' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-8f47242483c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m Pipeline = Pipeline([\n\u001b[0;32m     13\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[1;34m'pca'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRandomizedPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[1;34m'clf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     ])\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Pipeline' object is not callable"
     ]
    }
   ],
   "source": [
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "            \n",
    "sss = StratifiedShuffleSplit(labels_train, n_iter=1000, test_size=0.1, random_state=42)                       \n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('pca', RandomizedPCA()),\n",
    "        ('clf', RandomForestClassifier(random_state = 42))\n",
    "    ])\n",
    "\n",
    "\n",
    "x = [3]\n",
    "#d = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "\n",
    "d = ['entropy', 'gini']\n",
    "n = [450, 500, 650]\n",
    "#n = [10]\n",
    "s = [1]\n",
    "#s = [10]\n",
    "m = [7, 8, 9]\n",
    "#m = [2]\n",
    "\n",
    "param_grid = {'pca__n_components': x,\n",
    "              'clf__n_estimators': n,\n",
    "              'clf__min_samples_split': s,\n",
    "              'clf__max_depth': m,\n",
    "              'clf__criterion': d}\n",
    "\n",
    "\n",
    "clf = GridSearchCV(estimator = Pipeline, \n",
    "                             param_grid = param_grid,\n",
    "                             scoring = score,\n",
    "                             cv = sss)\n",
    "\n",
    "print clf\n",
    "\n",
    "print len(features_train)\n",
    "print len(labels_train)\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "print \"Training starts at: \", time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t0))\n",
    "clf.fit(features_train, labels_train)\n",
    "t1 = time.time()\n",
    "print \"Training ends at: \", time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t1))\n",
    "print \"Fitting is done.\"\n",
    "\n",
    "# print all the outcomes of interest\n",
    "get_outcomes(clf)\n",
    "\n",
    "my_features_list = features_list\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its result is not good, either. The value of recall never goes beyond 0.3 for different paramter settings. Its maximum value is 0.1429. Below is the best result:\n",
    "\n",
    "\n",
    "Training starts at:  2016-08-14 09:24:21\n",
    "Training ends at:  2016-08-14 15:19:49\n",
    "Fitting is done.\n",
    "Best parameters from the grid search:  {'pca__n_components': 3, 'clf__criterion': 'gini', 'clf__max_depth': 8, 'clf__n_estimators': 500, 'clf__min_samples_split': 1}\n",
    "\n",
    "Best Estimator Accuracy: 0.793103448276\n",
    "\n",
    "\n",
    "Recall Score: 0.142857142857\n",
    "\n",
    "\n",
    "Precision Score: 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8.3: PCA + DecisionTree + Pipeline + GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Pipeline' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-95417da137b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m Pipeline = Pipeline([\n\u001b[0;32m     12\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[1;34m'pca'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRandomizedPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[1;34m'clf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     ])\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Pipeline' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "#print data\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "#print labels\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "           \n",
    "sss = StratifiedShuffleSplit(labels_train, n_iter=1000, test_size=0.1, random_state=42)                       \n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('pca', RandomizedPCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 42))\n",
    "    ])\n",
    "\n",
    "\n",
    "x = [3]\n",
    "#d = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "\n",
    "d = ['entropy', 'gini']\n",
    "#n = [10]\n",
    "s = [1]\n",
    "#s = [10]\n",
    "m = [7, 8, 9]\n",
    "#m = [2]\n",
    "\n",
    "param_grid = {'pca__n_components': x,\n",
    "              'clf__min_samples_split': s,\n",
    "              'clf__max_depth': m,\n",
    "              'clf__criterion': d}\n",
    "\n",
    "\n",
    "clf = GridSearchCV(estimator = Pipeline, \n",
    "                             param_grid = param_grid,\n",
    "                             scoring = score,\n",
    "                             cv = sss)\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "print \"Training starts at: \", time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t0))\n",
    "clf.fit(features_train, labels_train)\n",
    "t1 = time.time()\n",
    "print \"Training ends at: \", time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t1))\n",
    "print \"Training time is \", time.strftime('%M:%S', time.localtime(t1-t0))\n",
    "print \"Fitting is done.\"\n",
    "\n",
    "# print all the outcomes of interest\n",
    "get_outcomes(clf)\n",
    "\n",
    "my_features_list = features_list\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its result is not good, either. The value of recall never goes beyond 0.3 for different paramter settings. Its maximum value is 0.1429. Below is the best result:\n",
    "\n",
    "\n",
    "Training starts at:  2016-08-14 18:59:47\n",
    "Training ends at:  2016-08-14 19:00:23\n",
    "Training time is  00:35\n",
    "Fitting is done.\n",
    "Best parameters from the grid search:  {'clf__criterion': 'gini', 'clf__max_depth': 8, 'pca__n_components': 3, 'clf__min_samples_split': 1}\n",
    "\n",
    "Best Estimator Accuracy: 0.724137931034\n",
    "\n",
    "\n",
    "Recall Score: 0.142857142857\n",
    "\n",
    "\n",
    "Precision Score: 0.333333333333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8.4: Manually Select Features + RandomForest + GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since using Pipeline + PCA, I am not able to get the good result. Then I decide to manually select features. So in the following code, two different features are selected from the feature lists each time. I am trying to find out which two features have the best and qualified result. The reason I choose two features each time is for reducing the calculation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_features_list = ['bonus', 'deferral_payments', 'director_fees', 'exercised_stock_options', \\\n",
    "                 'expenses', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', \\\n",
    "                 'salary']\n",
    "\n",
    "features_list = ['poi']\n",
    "\n",
    "f = open(\"poi_id_randomforest_loopfeatures_result.txt\", 'w')\n",
    "\n",
    "i = total_features_list[0]\n",
    "while (len(total_features_list) > 1):\n",
    "    features_list = ['poi']\n",
    "    features_list.append(i)\n",
    "    total_features_list.remove(i)\n",
    "    i = total_features_list[0]\n",
    "    for j in total_features_list:\n",
    "        features_list.append(j)\n",
    "        print features_list\n",
    "        f.write(str(features_list) + \"\\n\")\n",
    "        \n",
    "        data = featureFormat(my_dataset, features_list, remove_all_zeroes=False, sort_keys = True)\n",
    "\n",
    "#print data\n",
    "        labels, features = targetFeatureSplit(data)\n",
    "\n",
    "#print labels\n",
    "        features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# RandomForest\n",
    "        ran = RandomForestClassifier()\n",
    "        parameters = {\"n_estimators\" : [1, 2, 3, 4],\n",
    "              \"min_samples_split\" : [5, 10, 15],\n",
    "              \"max_depth\": [2, 3, 4, 5],\n",
    "              \"criterion\" : ['entropy', 'gini']}\n",
    "            \n",
    "        sss = StratifiedShuffleSplit(labels_train, n_iter=1000, test_size=0.1, random_state=42)                       \n",
    "        clf = GridSearchCV(ran,\n",
    "                   param_grid = parameters,\n",
    "                   scoring = score,\n",
    "                   cv = sss) \n",
    "\n",
    "        t0 = time.time()\n",
    "        t0_value = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t0))\n",
    "        print \"Training starts at: \", t0_value\n",
    "        f.write(\"Training starts at: \" + str(t0_value) + \"\\n\")\n",
    "        clf.fit(features_train, labels_train)\n",
    "        t1 = time.time()\n",
    "        t1_value = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t1))\n",
    "        f.write(\"Training ends at: \" + str(t1_value) + \"\\n\")\n",
    "        print \"Training ends at: \", t1_value\n",
    "\n",
    "# print all the outcomes of interest\n",
    "        get_outcomes2(clf, f)\n",
    "\n",
    "        features_list.remove(j)\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the result is not good. The best result is:\n",
    "\n",
    "['poi', 'loan_advances', 'restricted_stock']\n",
    "Training starts at: 2016-10-04 05:10:20\n",
    "Training ends at: 2016-10-04 05:21:11\n",
    "Best parameters from the grid search: {'min_samples_split': 5, 'n_estimators': 1, 'criterion': 'entropy', 'max_depth': 5}\n",
    "\n",
    "Best Estimator Accuracy: 0.689655172414\n",
    "\n",
    "\n",
    "Recall Score: 0.285714285714\n",
    "\n",
    "\n",
    "Recall Score: 0.285714285714\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8.5: Manually Select Features + DecisionTree + GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this solution, two different features are selected from the feature lists each time. And RandomForestClassifier is run. GridSearchCV() is used to get the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_features_list = ['bonus', 'deferral_payments', 'director_fees', 'exercised_stock_options', \\\n",
    "                 'expenses', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', \\\n",
    "                 'salary']\n",
    "\n",
    "features_list = ['poi']\n",
    "\n",
    "f = open(\"poi_id_dt_loopfeatures_result.txt\", 'w')\n",
    "\n",
    "i = total_features_list[0]\n",
    "while (len(total_features_list) > 1):\n",
    "    features_list = ['poi']\n",
    "    features_list.append(i)\n",
    "    total_features_list.remove(i)\n",
    "    i = total_features_list[0]\n",
    "    for j in total_features_list:\n",
    "        features_list.append(j)\n",
    "        print features_list\n",
    "        f.write(str(features_list) + \"\\n\")\n",
    "                 \n",
    "        data = featureFormat(my_dataset, features_list, remove_all_zeroes=False, sort_keys = True)\n",
    "\n",
    "#print data\n",
    "        labels, features = targetFeatureSplit(data)\n",
    "\n",
    "#print labels\n",
    "        features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "        dtc = DecisionTreeClassifier(random_state = 42)\n",
    "        parameters = {\"criterion\" : [\"gini\", \"entropy\"],\n",
    "                      \"splitter\" :   [\"best\", \"random\"]}  \n",
    "            \n",
    "        sss = StratifiedShuffleSplit(labels_train, n_iter=1000, test_size=0.1, random_state=42)                       \n",
    "        clf = GridSearchCV(dtc,\n",
    "                   param_grid = parameters,\n",
    "                   scoring = score,\n",
    "                   cv = sss) \n",
    "\n",
    "\n",
    "        t0 = time.time()\n",
    "        t0_value = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t0))\n",
    "        print \"Training starts at: \", t0_value\n",
    "        f.write(\"Training starts at: \" + str(t0_value) + \"\\n\")\n",
    "        clf.fit(features_train, labels_train)\n",
    "        t1 = time.time()\n",
    "        t1_value = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t1))\n",
    "        f.write(\"Training ends at: \" + str(t1_value) + \"\\n\")\n",
    "        print \"Training ends at: \", t1_value\n",
    "\n",
    "# print all the outcomes of interest\n",
    "        get_outcomes2(clf, f)\n",
    "\n",
    "        features_list.remove(j)\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this solution, I get some good results:\n",
    "    \n",
    "['poi', 'exercised_stock_options', 'salary']\n",
    "Training starts at: 2016-10-03 22:10:59\n",
    "Training ends at: 2016-10-03 22:11:10\n",
    "Best parameters from the grid search: {'splitter': 'best', 'criterion': 'entropy'}\n",
    "\n",
    "Best Estimator Accuracy: 0.758620689655\n",
    "\n",
    "\n",
    "Recall Score: 0.428571428571\n",
    "\n",
    "\n",
    "Recall Score: 0.42857142857    \n",
    "    \n",
    "    \n",
    "    \n",
    "['poi', 'expenses', 'salary']\n",
    "Training starts at: 2016-10-03 22:11:49\n",
    "Training ends at: 2016-10-03 22:11:59\n",
    "Best parameters from the grid search: {'splitter': 'random', 'criterion': 'gini'}\n",
    "\n",
    "Best Estimator Accuracy: 0.724137931034\n",
    "\n",
    "\n",
    "Recall Score: 0.428571428571\n",
    "\n",
    "\n",
    "Recall Score: 0.42857142857    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
